{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newsummary/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from bertopic.representation import LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "my_openai_api_key = \"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\"\n",
    "chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your representation model\n",
    "representation_model = LangChain(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the representation model in BERTopic on top of the default pipeline\n",
    "topic_model = BERTopic(representation_model=representation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x7f434eb985e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary 1: \",get_summary(final_text_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling tokenizer....\n",
      "calling model.....\n",
      "model loaded....\n",
      "urls =  ['https://www.governmentnews.com.au/queensland-says-no-to-new-olympic-stadium/']\n",
      "author1 =  <meta content=\"Judy Skatssoon\" name=\"author\"/>\n",
      "1\n",
      "6\n",
      "Queensland says it won’t be demolishing the Gabba and building a new Olympic stadium in Brisbane. This is despite the recommendations of an independent review of venues for the 2032 Games. The independent Sport Venue Review for Olympic Paralympic Games infrastructure was completed by an independent panel led by former Brisbane Lord Mayor Graham Quirk. It said a greenfield 50,000 seat stadium at Victoria Park would cost up to $3.4 billion but provide ‘an opportunity to deliver the best outcome’ The government said it would opt for a more modest enhancement of the existing facility.\n",
      "sentiment='neutral' aggressiveness=3 language='english' style='formal' title='NSW Health responds to a rise in Covid cases in light of new subvariant strains | news.com.au — Australia’s leading news site' author='Empty' date='2024-01-09 03:57:00+00:00' summary='Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. NSW Health is yet to release their fortnightly report for the New Year, but Covid activity has been on the rise since late December. Residents across one state have been urged to wear a mask if they show symptoms as a wave of infection sweeps parts of the country. South Wales has recorded its highest level of infection with more than 17 per cent of PCR tests returning positive results.'\n",
      "urls =  ['https://www.pm.gov.au/media/parents-and-economy-benefit-latest-reform']\n",
      "author1 =  None\n",
      "author =  Empty\n",
      "da =  empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 1024, but your input_length is only 817. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=408)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "Labor’s extension of Paid Parental Leave became law this week. More than 180,000 Australian families are expected to benefit. Every family with a new baby will be able to access a total of six months paid leave, shared between the two parents. It is the biggest boost to PPL since it was first introduced by the former Labor Government in 2011. A parental leave system that empowers the full and equal participation of women is good for business, good for families and good for our economy.\n",
      "sentiment='neutral' aggressiveness=3 language='english' style='formal' title='NSW Health responds to a rise in Covid cases in light of new subvariant strains | news.com.au — Australia’s leading news site' author='Empty' date='2024-01-09 03:57:00+00:00' summary='Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. NSW Health is yet to release their fortnightly report for the New Year, but Covid activity has been on the rise since late December. Residents across one state have been urged to wear a mask if they show symptoms as a wave of infection sweeps parts of the country. South Wales has recorded its highest level of infection with more than 17 per cent of PCR tests returning positive results.'\n",
      "urls =  ['https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc']\n",
      "author1 =  None\n",
      "author =  Empty\n",
      "1\n",
      "11\n",
      "Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. NSW Health is yet to release their fortnightly report for the New Year, but Covid activity has been on the rise since late December. Residents across one state have been urged to wear a mask if they show symptoms as a wave of infection sweeps parts of the country. South Wales has recorded its highest level of infection with more than 17 per cent of PCR tests returning positive results.\n",
      "sentiment='neutral' aggressiveness=3 language='english' style='informal' title='NSW Health responds to a rise in Covid cases in light of new subvariant strains | news.com.au — Australia’s leading news site' author='Empty' date='2024-01-09 03:57:00+00:00' summary='Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. NSW Health is yet to release their fortnightly report for the New Year, but Covid activity has been on the rise since late December. Residents across one state have been urged to wear a mask if they show symptoms as a wave of infection sweeps parts of the country. South Wales has recorded its highest level of infection with more than 17 per cent of PCR tests returning positive results.'\n"
     ]
    }
   ],
   "source": [
    "print(\"calling tokenizer....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"calling model.....\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"model loaded....\")\n",
    "\n",
    "def get_summary(final_text):\n",
    "  pipe_sum = pipeline(\n",
    "        'summarization',\n",
    "        model = model,\n",
    "        tokenizer = tokenizer\n",
    "        )\n",
    "  result = pipe_sum(final_text,\n",
    "                    max_length = 1024, \n",
    "                    min_length = 100,\n",
    "                    do_sample=False,\n",
    "                    truncation=True)\n",
    "  result = result[0]['summary_text']\n",
    "  \n",
    "  return result\n",
    "\n",
    "def text_preprocessing(u):\n",
    "  loaders = UnstructuredURLLoader(u)\n",
    "  data = loaders.load()\n",
    "  print(len(data))\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200\n",
    "      )\n",
    "  # As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "  docs = text_splitter.split_documents(data)\n",
    "  print(len(docs))\n",
    "  final_texts = \"\"\n",
    "  for i in range(0,len(docs)):\n",
    "    #print(i,\" : \", input_text[i].page_content)\n",
    "    final_texts = final_texts + docs[i].page_content\n",
    "    #print(\"final_texts : \", final_texts)\n",
    "  return final_texts\n",
    "\n",
    "# urls_1=[\n",
    "#     \"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"\n",
    "#     ]\n",
    "# urls_1= [\"https://www.governmentnews.com.au/risk-environment-rapidly-changing-government-cyber-honcho-warns/\"]\n",
    "\n",
    "# for i in range(0,len(urls)):\n",
    "#   print(urls[i])\n",
    "#   r = text_preprocessing(urls[i])\n",
    "# Schema\n",
    "# schema = {\n",
    "#     \"properties\": {\n",
    "#         \"sentiment\": {\"type\": \"string\"},\n",
    "#         \"aggressiveness\": {\"type\": \"integer\"},\n",
    "#         \"language\": {\"type\": \"string\"},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# def pop_default(s):\n",
    "#     s.pop('default')\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "class Tags(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "    # political_tendency: str\n",
    "    style: str = Field(..., enum = [\"formal\",\"informal\"])\n",
    "    title: str = Field(title)\n",
    "    author: str = Field(author) \n",
    "    date: str = Field(str(date))\n",
    "    summary: str  = Field(summary)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "default_urls = [\n",
    "    [\"https://www.governmentnews.com.au/queensland-says-no-to-new-olympic-stadium/\"],\n",
    "    [\"https://www.pm.gov.au/media/parents-and-economy-benefit-latest-reform\"],\n",
    "    [\"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"]\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_tagging_chain_pydantic(Tags, llm)\n",
    "for i in range(len(default_urls)):\n",
    "  print(\"urls = \", default_urls[i])\n",
    "  soup = BeautifulSoup(requests.get(default_urls[i][0]).content, 'html.parser')\n",
    "  t = soup.find('title')\n",
    "  title = t.get_text()\n",
    "  author1 = soup.find('meta', {'name': 'author'})\n",
    "  print(\"author1 = \", author1)\n",
    "  if author1 is not None:\n",
    "    author = author1[\"content\"]\n",
    "  else:\n",
    "    author = \"Empty\"\n",
    "    print(\"author = \", author)\n",
    "  datest = soup.find('meta', {'property': 'article:published_time'})\n",
    "  if datest is not None:\n",
    "    da = datest[\"content\"]\n",
    "    date = dparser.parse(da,fuzzy=True)\n",
    "    # date = str(datetime_obj.date())\n",
    "  else:\n",
    "    date = \"empty\"\n",
    "    print(\"da = \", date)\n",
    "  # print(\"date =\", str(date))\n",
    "  final_text_1 = text_preprocessing(default_urls[i])\n",
    "  summary = get_summary(final_text_1)\n",
    "  print(summary)\n",
    "  print(chain.run(summary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chain = create_tagging_chain(schema, llm)\n",
    "\n",
    "# final_text_1 = text_preprocessing(urls_1)\n",
    "# s1 = get_summary( final_text_1)\n",
    "# print(\"Summary 1: \",s1)\n",
    "\n",
    "# def retrieve_arttributes(urls_1):\n",
    "#     soup = BeautifulSoup(requests.get(urls_1[0]).content, 'html.parser')\n",
    "#     t = soup.find('title')\n",
    "#     title = t.get_text()\n",
    "#     author = soup.find('meta', {'name': 'author'})[\"content\"]\n",
    "#     date = soup.find('meta', {'property': 'article:published_time'})[\"content\"]\n",
    "#     final_text_1 = text_preprocessing(urls_1)\n",
    "#     summary = get_summary(final_text_1)\n",
    "\n",
    "#     print(chain.run(final_text_1))\n",
    "\n",
    "#     return title,author,date,summary\n",
    "\n",
    "# print(title.get_text(),author,date)\n",
    "\n",
    "\n",
    "\n",
    "# we add this to remove stopwords\n",
    "# vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "# model = BERTopic(\n",
    "#     vectorizer_model=vectorizer_model,\n",
    "#     language='english', calculate_probabilities=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "# topics, probs = model.fit_transform(s1)\n",
    "# my_openai_api_key = \"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\"\n",
    "# chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")\n",
    "# # Create your representation model\n",
    "# representation_model = LangChain(chain)\n",
    "# topic_model = BERTopic(representation_model=representation_model)\n",
    "# topic_model = BERTopic(tokenizer = tokenizer)\n",
    "# topic_model.fit_transform(s1)\n",
    "# model.get_topics()\n",
    "\n",
    "\n",
    "# urls_2 =[\n",
    "#     \"https://www.news.com.au/finance/business/other-industries/liquidator-of-collapsed-building-firm-alleges-director-was-loaned-nearly-1-million-in-company-money/news-story/493daf5d91f6a6b1267d542d6fa5b184\"\n",
    "#     ]\n",
    "# final_text_2 = text_preprocessing(urls_2)\n",
    "# s2 = get_summary(final_text_2)\n",
    "# print(\"Summary 2: \",s2)\n",
    "# print(chain.run(s2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling tokenizer....\n",
      "calling model.....\n",
      "model loaded....\n",
      "urls =  ['https://www.governmentnews.com.au/queensland-says-no-to-new-olympic-stadium/']\n",
      "author =  Judy Skatssoon\n",
      "1\n",
      "6\n",
      "Queensland says it won’t be demolishing the Gabba and building a new Olympic stadium in Brisbane. This is despite the recommendations of an independent review of venues for the 2032 Games. The independent Sport Venue Review for Olympic Paralympic Games infrastructure was completed by an independent panel led by former Brisbane Lord Mayor Graham Quirk. It said a greenfield 50,000 seat stadium at Victoria Park would cost up to $3.4 billion but provide ‘an opportunity to deliver the best outcome’ The government said it would opt for a more modest enhancement of the existing facility.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "4 validation errors for Tags\nsentiment\n  field required (type=value_error.missing)\naggressiveness\n  field required (type=value_error.missing)\nlanguage\n  field required (type=value_error.missing)\nstyle\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[102], line 111\u001b[0m\n\u001b[1;32m    109\u001b[0m   summary \u001b[38;5;241m=\u001b[39m get_summary(final_text_1)\n\u001b[1;32m    110\u001b[0m   \u001b[38;5;28mprint\u001b[39m(summary)\n\u001b[0;32m--> 111\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# chain = create_tagging_chain(schema, llm)\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# final_text_1 = text_preprocessing(urls_1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# print(\"Summary 2: \",s2)\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# print(chain.run(s2))\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/base.py:545\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    544\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    546\u001b[0m         _output_key\n\u001b[1;32m    547\u001b[0m     ]\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    551\u001b[0m         _output_key\n\u001b[1;32m    552\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/base.py:378\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    347\u001b[0m \n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    371\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    376\u001b[0m }\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/base.py:163\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    162\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    164\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/base.py:153\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    152\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    156\u001b[0m     )\n\u001b[1;32m    158\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    159\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    160\u001b[0m     )\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/llm.py:104\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    100\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    101\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    102\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    103\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate([inputs], run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_outputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/llm.py:258\u001b[0m, in \u001b[0;36mLLMChain.create_outputs\u001b[0;34m(self, llm_result)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         {\n\u001b[1;32m    261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_parser\u001b[38;5;241m.\u001b[39mparse_result(generation),\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[1;32m    263\u001b[0m         }\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[1;32m    265\u001b[0m     ]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain/chains/llm.py:261\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_outputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_result: LLMResult) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    257\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create outputs from response.\"\"\"\u001b[39;00m\n\u001b[1;32m    258\u001b[0m     result \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# Get the text of the top generated string.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         {\n\u001b[0;32m--> 261\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    262\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfull_generation\u001b[39m\u001b[38;5;124m\"\u001b[39m: generation,\n\u001b[1;32m    263\u001b[0m         }\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m generation \u001b[38;5;129;01min\u001b[39;00m llm_result\u001b[38;5;241m.\u001b[39mgenerations\n\u001b[1;32m    265\u001b[0m     ]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_final_only:\n\u001b[1;32m    267\u001b[0m         result \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: r[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]} \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/langchain_core/output_parsers/openai_functions.py:204\u001b[0m, in \u001b[0;36mPydanticOutputFunctionsParser.parse_result\u001b[0;34m(self, result, partial)\u001b[0m\n\u001b[1;32m    202\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mparse_result(result)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs_only:\n\u001b[0;32m--> 204\u001b[0m     pydantic_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpydantic_schema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_result\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     fn_name \u001b[38;5;241m=\u001b[39m _result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/pydantic/v1/main.py:549\u001b[0m, in \u001b[0;36mBaseModel.parse_raw\u001b[0;34m(cls, b, content_type, encoding, proto, allow_pickle)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(e, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/pydantic/v1/main.py:526\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m    524\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected dict not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    525\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/newsummary/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 4 validation errors for Tags\nsentiment\n  field required (type=value_error.missing)\naggressiveness\n  field required (type=value_error.missing)\nlanguage\n  field required (type=value_error.missing)\nstyle\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"calling tokenizer....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"calling model.....\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"model loaded....\")\n",
    "\n",
    "def get_summary(final_text):\n",
    "  pipe_sum = pipeline(\n",
    "        'summarization',\n",
    "        model = model,\n",
    "        tokenizer = tokenizer\n",
    "        )\n",
    "  result = pipe_sum(final_text,\n",
    "                    max_length = 1024, \n",
    "                    min_length = 100,\n",
    "                    do_sample=False,\n",
    "                    truncation=True)\n",
    "  result = result[0]['summary_text']\n",
    "  \n",
    "  return result\n",
    "\n",
    "def text_preprocessing(u):\n",
    "  loaders = UnstructuredURLLoader(u)\n",
    "  data = loaders.load()\n",
    "  print(len(data))\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200\n",
    "      )\n",
    "  # As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "  docs = text_splitter.split_documents(data)\n",
    "  print(len(docs))\n",
    "  final_texts = \"\"\n",
    "  for i in range(0,len(docs)):\n",
    "    #print(i,\" : \", input_text[i].page_content)\n",
    "    final_texts = final_texts + docs[i].page_content\n",
    "    #print(\"final_texts : \", final_texts)\n",
    "  return final_texts\n",
    "\n",
    "# urls_1=[\n",
    "#     \"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"\n",
    "#     ]\n",
    "# urls_1= [\"https://www.governmentnews.com.au/risk-environment-rapidly-changing-government-cyber-honcho-warns/\"]\n",
    "\n",
    "# for i in range(0,len(urls)):\n",
    "#   print(urls[i])\n",
    "#   r = text_preprocessing(urls[i])\n",
    "# Schema\n",
    "# schema = {\n",
    "#     \"properties\": {\n",
    "#         \"sentiment\": {\"type\": \"string\"},\n",
    "#         \"aggressiveness\": {\"type\": \"integer\"},\n",
    "#         \"language\": {\"type\": \"string\"},\n",
    "#     }\n",
    "# }\n",
    "\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import dateutil.parser as dparser\n",
    "\n",
    "class Tags(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "    # political_tendency: str\n",
    "    style: str = Field(..., enum = [\"formal\",\"informal\"])\n",
    "    title: str = Field(title)\n",
    "    author: str = Field(author, \n",
    "\n",
    "\n",
    "\n",
    "    date: str = Field(str(date))\n",
    "    summary: str  = Field(summary)\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "default_urls = [\n",
    "    [\"https://www.governmentnews.com.au/queensland-says-no-to-new-olympic-stadium/\"],\n",
    "    [\"https://www.pm.gov.au/media/parents-and-economy-benefit-latest-reform\"],\n",
    "    [\"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"]\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_tagging_chain_pydantic(Tags, llm)\n",
    "for i in range(len(default_urls)):\n",
    "  print(\"urls = \", default_urls[i])\n",
    "  soup = BeautifulSoup(requests.get(default_urls[i][0]).content, 'html.parser')\n",
    "  t = soup.find('title')\n",
    "  title = t.get_text()\n",
    "  author1 = soup.find('meta', {'name': 'author'})\n",
    "  # print(\"author = \", author)\n",
    "  if author1 is not None:\n",
    "    author = author1[\"content\"]\n",
    "  else:\n",
    "    author = \"Empty\"\n",
    "  datest = soup.find('meta', {'property': 'article:published_time'})[\"content\"]\n",
    "  datetime_obj = dparser.parse(datest,fuzzy=True)\n",
    "  date = str(datetime_obj.date())\n",
    "  # print(\"date =\", str(date))\n",
    "  final_text_1 = text_preprocessing(default_urls[i])\n",
    "  summary = get_summary(final_text_1)\n",
    "  print(summary)\n",
    "  print(chain.run(summary))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# chain = create_tagging_chain(schema, llm)\n",
    "\n",
    "# final_text_1 = text_preprocessing(urls_1)\n",
    "# s1 = get_summary( final_text_1)\n",
    "# print(\"Summary 1: \",s1)\n",
    "\n",
    "# def retrieve_arttributes(urls_1):\n",
    "#     soup = BeautifulSoup(requests.get(urls_1[0]).content, 'html.parser')\n",
    "#     t = soup.find('title')\n",
    "#     title = t.get_text()\n",
    "#     author = soup.find('meta', {'name': 'author'})[\"content\"]\n",
    "#     date = soup.find('meta', {'property': 'article:published_time'})[\"content\"]\n",
    "#     final_text_1 = text_preprocessing(urls_1)\n",
    "#     summary = get_summary(final_text_1)\n",
    "\n",
    "#     print(chain.run(final_text_1))\n",
    "\n",
    "#     return title,author,date,summary\n",
    "\n",
    "# print(title.get_text(),author,date)\n",
    "\n",
    "\n",
    "\n",
    "# we add this to remove stopwords\n",
    "# vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "# model = BERTopic(\n",
    "#     vectorizer_model=vectorizer_model,\n",
    "#     language='english', calculate_probabilities=True,\n",
    "#     verbose=True\n",
    "# )\n",
    "# topics, probs = model.fit_transform(s1)\n",
    "# my_openai_api_key = \"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\"\n",
    "# chain = load_qa_chain(OpenAI(temperature=0, openai_api_key=my_openai_api_key), chain_type=\"stuff\")\n",
    "# # Create your representation model\n",
    "# representation_model = LangChain(chain)\n",
    "# topic_model = BERTopic(representation_model=representation_model)\n",
    "# topic_model = BERTopic(tokenizer = tokenizer)\n",
    "# topic_model.fit_transform(s1)\n",
    "# model.get_topics()\n",
    "\n",
    "\n",
    "# urls_2 =[\n",
    "#     \"https://www.news.com.au/finance/business/other-industries/liquidator-of-collapsed-building-firm-alleges-director-was-loaned-nearly-1-million-in-company-money/news-story/493daf5d91f6a6b1267d542d6fa5b184\"\n",
    "#     ]\n",
    "# final_text_2 = text_preprocessing(urls_2)\n",
    "# s2 = get_summary(final_text_2)\n",
    "# print(\"Summary 2: \",s2)\n",
    "# print(chain.run(s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Liquidator of collapsed building firm alleges director was loaned nearly $1 million in company money before the business collapsed. Building boss received a loan for nearly £1 million while his customers have been left with nothing, according to a public liquidator’s report. The regulator, ASIC, has not taken action against the director.Builder in major national franchise collapses. Gold, oil stocks weigh down sharemarket. Deals of the Week is a weekly quiz to test your knowledge of events in the news.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'negative', 'aggressiveness': 1, 'language': 'English'}\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "schema = {\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\"type\": \"string\"},\n",
    "        \"aggressiveness\": {\"type\": \"integer\"},\n",
    "        \"language\": {\"type\": \"string\"},\n",
    "        \"style\": {\"type\":\"string\"}\n",
    "    }\n",
    "}\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "chain = create_tagging_chain(schema, llm)\n",
    "# print(chain.run(s1))\n",
    "print(chain.run(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tags(sentiment='neutral', aggressiveness=3, language='english', political_tendency='unknown', style='informal', author='unknown')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "class Tags(BaseModel):\n",
    "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])\n",
    "    aggressiveness: int = Field(\n",
    "        ...,\n",
    "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
    "        enum=[1, 2, 3, 4, 5],\n",
    "    )\n",
    "    language: str = Field(\n",
    "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
    "    )\n",
    "    political_tendency: str\n",
    "    style: str = Field(..., enum = [\"formal\",\"informal\"])\n",
    "    author: str\n",
    "\n",
    "chain = create_tagging_chain_pydantic(Tags, llm)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "def retrieve_arttributes():\n",
    "    soup = BeautifulSoup(requests.get(s).content, 'html.parser')\n",
    "    # title = soup.find('title')\n",
    "    author = soup.find('meta', {'name': 'author'})[\"content\"]\n",
    "    date = soup.find('meta', {'property': 'article:published_time'})[\"content\"]\n",
    "    return soup.title.extract(),author,date\n",
    "res = chain.run(s1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calling tokenizer....\n",
      "calling model.....\n",
      "model loaded....\n",
      "1\n",
      "11\n",
      "Summary 1:  Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. Residents across one state have been urged to wear a mask if they show Covid symptoms as a new wave of infection sweeps parts of the country. New South Wales has recorded its highest level of the virus in a year, with more than 17 per cent of PCR tests returning positive results. NSW Health is yet to release their report for the New Year, with increasing numbers of cases across the state.\n",
      "1\n",
      "13\n",
      "Summary 2:  Liquidator of collapsed building firm alleges director was loaned nearly $1 million in company money according to public liquidator’s report. The regulator, ASIC, has not taken action against the director under the Corporations Act.Iconic Aussie retailer to close forever. Selena Gomez’S $3 billion beauty move. The Australian dollar dips before blockbuster US meeting. The Aussie dollar falls to a record low against the US dollar. The Sydney Opera House is set to open for its 50th anniversary.\n"
     ]
    }
   ],
   "source": [
    "print(\"calling tokenizer....\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"calling model.....\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "print(\"model loaded....\")\n",
    "\n",
    "def get_summary(final_text):\n",
    "  pipe_sum = pipeline(\n",
    "        'summarization',\n",
    "        model = model,\n",
    "        tokenizer = tokenizer\n",
    "        )\n",
    "  result = pipe_sum(final_text,\n",
    "                    max_length = 1024, \n",
    "                    min_length = 100,\n",
    "                    do_sample=False,\n",
    "                    truncation=True)\n",
    "  result = result[0]['summary_text']\n",
    "  \n",
    "  return result\n",
    "\n",
    "def text_preprocessing(u):\n",
    "  loaders = UnstructuredURLLoader(u)\n",
    "  data = loaders.load()\n",
    "  print(len(data))\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200\n",
    "      )\n",
    "  # As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "  docs = text_splitter.split_documents(data)\n",
    "  print(len(docs))\n",
    "  final_texts = \"\"\n",
    "  for i in range(0,len(docs)):\n",
    "    #print(i,\" : \", input_text[i].page_content)\n",
    "    final_texts = final_texts + docs[i].page_content\n",
    "    #print(\"final_texts : \", final_texts)\n",
    "  return final_texts\n",
    "\n",
    "urls_1=[\n",
    "    \"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"\n",
    "    ]\n",
    "\n",
    "# for i in range(0,len(urls)):\n",
    "#   print(urls[i])\n",
    "#   r = text_preprocessing(urls[i])\n",
    "\n",
    "final_text_1 = text_preprocessing(urls_1)\n",
    "#get_summary( final_text_1)\n",
    "print(\"Summary 1: \",get_summary(final_text_1))\n",
    "\n",
    "urls_2 =[\n",
    "    \"https://www.news.com.au/finance/business/other-industries/liquidator-of-collapsed-building-firm-alleges-director-was-loaned-nearly-1-million-in-company-money/news-story/493daf5d91f6a6b1267d542d6fa5b184\"\n",
    "    ]\n",
    "final_text_2 = text_preprocessing(urls_2)\n",
    "print(\"Summary 2: \",get_summary(final_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL : <class 'list'>    ['https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc']\n",
      "1\n",
      "11\n",
      "Summary 1:  Two new subvariants of Covid are ripping through multiple states, forcing health officials to call for masks to stem transmission. Residents across one state have been urged to wear a mask if they show Covid symptoms as a new wave of infection sweeps parts of the country. New South Wales has recorded its highest level of the virus in a year, with more than 17 per cent of PCR tests returning positive results. NSW Health is yet to release their report for the New Year, with increasing numbers of cases across the state.\n",
      "1\n",
      "13\n",
      "Summary 2:  Liquidator of collapsed building firm alleges director was loaned nearly $1 million in company money according to public liquidator’s report. The regulator, ASIC, has not taken action against the director under the Corporations Act.Iconic Aussie retailer to close forever. Selena Gomez’S $3 billion beauty move. The Australian dollar dips before blockbuster US meeting. The Aussie dollar falls to a record low against the US dollar. The Sydney Opera House is set to open for its 50th anniversary.\n"
     ]
    }
   ],
   "source": [
    "# print(\"calling tokenizer....\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# print(\"calling model.....\")\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "# print(\"model loaded....\")\n",
    "\n",
    "def get_summary(final_text):\n",
    "#   pipe_sum = pipeline(\n",
    "#         'summarization',\n",
    "#         model = model,\n",
    "#         tokenizer = tokenizer\n",
    "#         )\n",
    "  summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "  min_len = 100\n",
    "  result = summarizer(final_text,\n",
    "                    max_length = 1024, \n",
    "                    min_length = min_len,\n",
    "                    do_sample=False,\n",
    "                    truncation=True)\n",
    "  result = result[0]['summary_text']\n",
    "  \n",
    "  return result\n",
    "\n",
    "def text_preprocessing(u):\n",
    "  loaders = UnstructuredURLLoader(u)\n",
    "  data = loaders.load()\n",
    "  print(len(data))\n",
    "  text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200\n",
    "      )\n",
    "  # As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "  docs = text_splitter.split_documents(data)\n",
    "  print(len(docs))\n",
    "  final_texts = \"\"\n",
    "  for i in range(0,len(docs)):\n",
    "    #print(i,\" : \", input_text[i].page_content)\n",
    "    final_texts = final_texts + docs[i].page_content\n",
    "    #print(\"final_texts : \", final_texts)\n",
    "  return final_texts\n",
    "\n",
    "urls_1=[\n",
    "    \"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"\n",
    "    ]\n",
    "\n",
    "# for i in range(0,len(urls)):\n",
    "#   print(urls[i])\n",
    "#   r = text_preprocessing(urls[i])\n",
    "print(\"URL :\", type(urls_1), \"  \", urls_1)\n",
    "final_text_1 = text_preprocessing(urls_1)\n",
    "#get_summary( final_text_1)\n",
    "print(\"Summary 1: \",get_summary(final_text_1))\n",
    "\n",
    "urls_2 =[\n",
    "    \"https://www.news.com.au/finance/business/other-industries/liquidator-of-collapsed-building-firm-alleges-director-was-loaned-nearly-1-million-in-company-money/news-story/493daf5d91f6a6b1267d542d6fa5b184\"\n",
    "    ]\n",
    "final_text_2 = text_preprocessing(urls_2)\n",
    "print(\"Summary 2: \",get_summary(final_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "min_len = int(\"100\")\n",
    "\n",
    "print(type(min_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "if min_len is None:\n",
    "    print(type(min_len))\n",
    "else:\n",
    "    print(type(min_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "min = \"100\"\n",
    "\n",
    "print(int(min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from transformers import pipeline, BartTokenizerFast\n",
    "from transformers import pipeline\n",
    "\n",
    "import os,time\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"News Article Summarizer 📰\")\n",
    "st.write(\"Uncover Insights, Save Time ⏳\")\n",
    "\n",
    "st.sidebar.title(\"News Article URLs\")\n",
    "\n",
    "\n",
    "def get_summary(final_text):\n",
    "   summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "   result = summarizer(final_text,\n",
    "                    max_length = 1024, \n",
    "                    min_length = 100,\n",
    "                    do_sample=False,\n",
    "                    truncation=True)\n",
    "   result = result[0]['summary_text']\n",
    "   st.subheader(f\"Summary for Article:\")\n",
    "   st.write(result)\n",
    "#    return result\n",
    "\n",
    "\n",
    "def text_preprocessing(u):\n",
    "   loaders = UnstructuredURLLoader(u)\n",
    "   data = loaders.load()\n",
    "   print(len(data))\n",
    "   text_splitter = RecursiveCharacterTextSplitter(\n",
    "      chunk_size=1000,\n",
    "      chunk_overlap=200\n",
    "      )\n",
    "   # As data is of type documents we can directly use split_documents over split_text in order to get the chunks.\n",
    "   docs = text_splitter.split_documents(data)\n",
    "   print(len(docs))\n",
    "   final_texts = \"\"\n",
    "   for i in range(0,len(docs)):\n",
    "    #print(i,\" : \", input_text[i].page_content)\n",
    "    final_texts = final_texts + docs[i].page_content\n",
    "    #print(\"final_texts : \", final_texts)\n",
    "   return final_texts\n",
    "\n",
    "\n",
    "# urls_1=[\n",
    "#     \"https://www.news.com.au/world/coronavirus/health/wear-a-mask-nsw-health-responds-to-a-rise-in-cases-in-light-of-new-subvariant-strains/news-story/90cad04f2a329d8730871c00b3dd00cc\"\n",
    "#     ]\n",
    "url_input = st.text_area(\"URL(s)\", height=100, help=\"Enter one or more news article URLs separated by line breaks.\")\n",
    "urls = []\n",
    "for url in url_input.split(\"\\n\"):\n",
    "        if url.strip() != \"\":\n",
    "            urls.append([url])\n",
    "print(\"url: \", urls)\n",
    "\n",
    "# urls = []\n",
    "# for i in range(2):\n",
    "#    url = st.text_input(f\"URL {0+i}\")\n",
    "#    urls.append([url])\n",
    "#    print(urls[i])\n",
    "if st.button(\"Process URL\"):\n",
    "    for i in range(len(urls)):\n",
    "       r = text_preprocessing(urls[i])\n",
    "       get_summary(r)\n",
    "    #    st.subheader(f\"Summary for Article {i+1}:\")\n",
    "    #    print(\"Summary 1: \",s)\n",
    "    #    st.write(s)\n",
    "       st.write(f\"source: {urls[i]}\")\n",
    "\n",
    "# final_text_1 = text_preprocessing(urls_1)\n",
    "#get_summary( final_text_1)\n",
    "# print(\"Summary 1: \",get_summary(r))\n",
    "\n",
    "# urls_2 =[\n",
    "#     \"https://www.news.com.au/finance/business/other-industries/liquidator-of-collapsed-building-firm-alleges-director-was-loaned-nearly-1-million-in-company-money/news-story/493daf5d91f6a6b1267d542d6fa5b184\"\n",
    "#     ]\n",
    "# final_text_2 = text_preprocessing(urls_2)\n",
    "# print(\"Summary 2: \",get_summary(final_text_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sidebar title with tooltip\n",
    "st.sidebar.title(\"About the News Article Summarizer ℹ️\")\n",
    "st.sidebar.write(\n",
    "    \"Use the sidebar to learn more about the tool and get started!\"\n",
    ")\n",
    "\n",
    "# Informational description in the sidebar\n",
    "st.sidebar.header(\"About the News Article Summarizer:\")\n",
    "st.sidebar.write(\n",
    "    \"The News Article Summarizer is a powerful tool designed to provide quick and accurate summaries of news articles. Developed as a proof of concept by a data science researcher, this tool utilizes advanced natural language processing (NLP) techniques to extract key insights and main points from lengthy articles, saving users valuable time and effort.\"\n",
    ")\n",
    "\n",
    "# Model information in the sidebar\n",
    "st.sidebar.header(\"Underlying Model: facebook/bart-large-cnn\")\n",
    "st.sidebar.write(\n",
    "    \"The News Article Summarizer is powered by the 'facebook/bart-large-cnn' model, a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture. This model is specifically trained for summarization tasks and excels at generating high-quality summaries by leveraging both auto-regressive and masked language modeling techniques.\"\n",
    ")\n",
    "\n",
    "# Features section in the sidebar\n",
    "st.sidebar.header(\"Powerfulness and Limitations:\")\n",
    "st.sidebar.write(\n",
    "    \"### Powerfulness:\\n\"\n",
    "    \" - **Efficient Summarization**: Instantly summarize news articles with high accuracy.\\n\"\n",
    "    \" - **Multi-URL Support**: Summarize multiple articles at once for increased productivity.\\n\"\n",
    "    \" - **Customizable**: Fine-tune summarization parameters to meet specific requirements.\\n\\n\"\n",
    "    \"### Limitations:\\n\"\n",
    "    \" - **Length Constraints**: Limited to summarizing articles within a certain length due to memory constraints.\\n\"\n",
    "    \" - **Content Understanding**: May not accurately summarize articles with complex or ambiguous content.\\n\"\n",
    "    \" - **Dependency on Training Data**: Performance may vary depending on the quality and diversity of the training data.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Here's the information about the News Article Summarizer tool, including details about the model `facebook/bart-large-cnn`, its technique, powerfulness, and limitations, presented in the sidebar:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set up pipeline for summarization using BART model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Sidebar title with tooltip\n",
    "st.sidebar.title(\"About the News Article Summarizer ℹ️\")\n",
    "st.sidebar.write(\n",
    "    \"Use the sidebar to learn more about the tool and get started!\"\n",
    ")\n",
    "\n",
    "# Informational description in the sidebar\n",
    "st.sidebar.header(\"About the News Article Summarizer:\")\n",
    "st.sidebar.write(\n",
    "    \"The News Article Summarizer is a proof of concept tool developed by a data science researcher to demonstrate the capabilities of natural language processing (NLP) in summarizing news articles. The tool utilizes the `facebook/bart-large-cnn` model, a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture, known for its effectiveness in text summarization tasks.\"\n",
    ")\n",
    "\n",
    "# Model details in the sidebar\n",
    "st.sidebar.header(\"Model Details:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is a pre-trained transformer-based model fine-tuned specifically for summarization tasks. It leverages the BART architecture, which incorporates both bidirectional and auto-regressive mechanisms to generate high-quality summaries of input text. This model is trained on a large corpus of news articles, enabling it to effectively capture and summarize key information.\"\n",
    ")\n",
    "\n",
    "# Powerfulness of the model in the sidebar\n",
    "st.sidebar.header(\"Powerfulness:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is known for its powerful summarization capabilities, capable of generating concise and coherent summaries of news articles with high accuracy. It can effectively distill key insights and main points from lengthy articles, providing users with quick and informative summaries to aid in their understanding of the content.\"\n",
    ")\n",
    "\n",
    "# Limitations of the model in the sidebar\n",
    "st.sidebar.header(\"Limitations:\")\n",
    "st.sidebar.write(\n",
    "    \"While the `facebook/bart-large-cnn` model is powerful, it also has its limitations. Due to the constraints of the underlying architecture and computational resources, the model may struggle with extremely long articles or those containing complex structures. Additionally, the quality of the generated summaries may vary depending on factors such as the domain and style of the input articles.\"\n",
    ")\n",
    "```\n",
    "\n",
    "This code adds detailed information about the News Article Summarizer tool, including insights into the `facebook/bart-large-cnn` model, its technique, powerfulness, and limitations, presented in the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sidebar title with tooltip\n",
    "st.sidebar.title(\"About the News Article Summarizer ℹ️\")\n",
    "st.sidebar.write(\n",
    "    \"Use the sidebar to learn more about the tool and get started!\"\n",
    ")\n",
    "\n",
    "# Informational description in the sidebar\n",
    "st.sidebar.header(\"About the News Article Summarizer:\")\n",
    "st.sidebar.write(\n",
    "    \"As a data science researcher, I have developed this tool as a proof of concept to demonstrate the capabilities of the Facebook BART-Large-CNN model in summarizing news articles. \"\n",
    "    \"The BART (Bidirectional and Auto-Regressive Transformers) model is a powerful transformer-based model that excels in natural language processing tasks such as text summarization. \"\n",
    "    \"By leveraging advanced techniques in NLP, the News Article Summarizer can quickly extract key insights and main points from news articles, saving you time and effort.\"\n",
    ")\n",
    "\n",
    "# Model details in the sidebar\n",
    "st.sidebar.header(\"Model Details:\")\n",
    "st.sidebar.write(\n",
    "    \"Model: Facebook BART-Large-CNN\\n\"\n",
    "    \"Technique: Bidirectional and Auto-Regressive Transformers (BART)\\n\"\n",
    "    \"Powerfulness: Excellent at summarizing news articles and extracting key information.\\n\"\n",
    "    \"Limitations: Limited by the maximum input length for summarization and may struggle with complex or nuanced language.\"\n",
    ")\n",
    "\n",
    "# Features section in the sidebar\n",
    "st.sidebar.header(\"Features:\")\n",
    "st.sidebar.write(\n",
    "    \"- **Efficient Summarization**: Instantly summarize news articles with just a click of a button.\\n\"\n",
    "    \"- **Accurate Insights**: Extract main points and key information from lengthy articles with high accuracy.\\n\"\n",
    "    \"- **Multi-URL Support**: Summarize multiple articles at once by entering their URLs separated by line breaks.\\n\"\n",
    "    \"- **Easy to Use**: User-friendly interface makes it simple for anyone to utilize the tool without any technical expertise.\"\n",
    ")\n",
    "\n",
    "# How It Works section in the sidebar\n",
    "st.sidebar.header(\"How It Works:\")\n",
    "st.sidebar.write(\n",
    "    \"1. **Enter URL(s)**: Provide the URL(s) of the news article(s) you want to summarize.\\n\"\n",
    "    \"2. **Generate Summary**: Click the 'Summarize' button to initiate the summarization process.\\n\"\n",
    "    \"3. **Review Results**: Read the concise summaries generated for each article and gain quick insights.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Certainly! Here's the updated information in the sidebar, including details about the Hugging Face interface and the developers of the `facebook/bart-large-cnn` model:\n",
    "\n",
    "```python\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set up pipeline for summarization using BART model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Sidebar title with tooltip\n",
    "st.sidebar.title(\"About the News Article Summarizer ℹ️\")\n",
    "st.sidebar.write(\n",
    "    \"Use the sidebar to learn more about the tool and get started!\"\n",
    ")\n",
    "\n",
    "# Informational description in the sidebar\n",
    "st.sidebar.header(\"About the News Article Summarizer:\")\n",
    "st.sidebar.write(\n",
    "    \"The News Article Summarizer is a proof of concept tool developed by a data science researcher to demonstrate the capabilities of natural language processing (NLP) in summarizing news articles. The tool utilizes the `facebook/bart-large-cnn` model, a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture, known for its effectiveness in text summarization tasks.\"\n",
    ")\n",
    "\n",
    "# Hugging Face interface details in the sidebar\n",
    "st.sidebar.header(\"Hugging Face Interface:\")\n",
    "st.sidebar.write(\n",
    "    \"The tool leverages the Hugging Face interface for accessing and utilizing pre-trained transformer-based models. Hugging Face provides a user-friendly platform and API for working with state-of-the-art NLP models, enabling developers and researchers to easily integrate these models into their applications and projects.\"\n",
    ")\n",
    "\n",
    "# Model details in the sidebar\n",
    "st.sidebar.header(\"Model Details:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is a pre-trained transformer-based model fine-tuned specifically for summarization tasks. It leverages the BART architecture, which incorporates both bidirectional and auto-regressive mechanisms to generate high-quality summaries of input text. This model is trained on a large corpus of news articles, enabling it to effectively capture and summarize key information.\"\n",
    ")\n",
    "\n",
    "# Powerfulness of the model in the sidebar\n",
    "st.sidebar.header(\"Powerfulness:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is known for its powerful summarization capabilities, capable of generating concise and coherent summaries of news articles with high accuracy. It can effectively distill key insights and main points from lengthy articles, providing users with quick and informative summaries to aid in their understanding of the content.\"\n",
    ")\n",
    "\n",
    "# Developers of the model in the sidebar\n",
    "st.sidebar.header(\"Model Developers:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model was developed by Facebook AI Research (FAIR), a research organization within Facebook dedicated to advancing the field of artificial intelligence and machine learning. The model is part of the broader effort to create and deploy state-of-the-art NLP models for various language understanding tasks.\"\n",
    ")\n",
    "\n",
    "# Limitations of the model in the sidebar\n",
    "st.sidebar.header(\"Limitations:\")\n",
    "st.sidebar.write(\n",
    "    \"While the `facebook/bart-large-cnn` model is powerful, it also has its limitations. Due to the constraints of the underlying architecture and computational resources, the model may struggle with extremely long articles or those containing complex structures. Additionally, the quality of the generated summaries may vary depending on factors such as the domain and style of the input articles.\"\n",
    ")\n",
    "```\n",
    "\n",
    "This updated code provides comprehensive information about the News Article Summarizer tool, including insights into the Hugging Face interface and the developers of the `facebook/bart-large-cnn` model, presented in the sidebar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "\n",
    "# Set up pipeline for summarization using BART model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Sidebar title with tooltip\n",
    "st.sidebar.title(\"About the News Article Summarizer ℹ️\")\n",
    "st.sidebar.write(\n",
    "    \"Use the sidebar to learn more about the tool and get started!\"\n",
    ")\n",
    "\n",
    "# Informational description in the sidebar\n",
    "st.sidebar.header(\"About the News Article Summarizer:\")\n",
    "st.sidebar.write(\n",
    "    \"The News Article Summarizer is a proof of concept tool developed by a data science researcher to demonstrate the capabilities of natural language processing (NLP) in summarizing news articles. The tool utilizes the `facebook/bart-large-cnn` model, a variant of the BART (Bidirectional and Auto-Regressive Transformers) architecture, known for its effectiveness in text summarization tasks.\"\n",
    ")\n",
    "\n",
    "# Hugging Face interface details in the sidebar\n",
    "st.sidebar.header(\"Hugging Face Interface:\")\n",
    "st.sidebar.write(\n",
    "    \"The tool leverages the Hugging Face interface for accessing and utilizing pre-trained transformer-based models. Hugging Face provides a user-friendly platform and API for working with state-of-the-art NLP models, enabling developers and researchers to easily integrate these models into their applications and projects.\"\n",
    ")\n",
    "\n",
    "# Model details in the sidebar\n",
    "st.sidebar.header(\"Model Details:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is a pre-trained transformer-based model fine-tuned specifically for summarization tasks. It leverages the BART architecture, which incorporates both bidirectional and auto-regressive mechanisms to generate high-quality summaries of input text. This model is trained on a large corpus of news articles, enabling it to effectively capture and summarize key information.\"\n",
    ")\n",
    "\n",
    "# Powerfulness of the model in the sidebar\n",
    "st.sidebar.header(\"Powerfulness:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model is known for its powerful summarization capabilities, capable of generating concise and coherent summaries of news articles with high accuracy. It can effectively distill key insights and main points from lengthy articles, providing users with quick and informative summaries to aid in their understanding of the content.\"\n",
    ")\n",
    "\n",
    "# Limitations of the model in the sidebar\n",
    "st.sidebar.header(\"Limitations:\")\n",
    "st.sidebar.write(\n",
    "    \"While the `facebook/bart-large-cnn` model is powerful, it also has its limitations. Due to the constraints of the underlying architecture and computational resources, the model may struggle with extremely long articles or those containing complex structures. Additionally, the quality of the generated summaries may vary depending on factors such as the domain and style of the input articles.\"\n",
    ")\n",
    "\n",
    "# Further optimization and use cases in the sidebar\n",
    "st.sidebar.header(\"Further Optimization and Use Cases:\")\n",
    "st.sidebar.write(\n",
    "    \"The `facebook/bart-large-cnn` model can be further optimized and customized for specific use cases within the Australian government, particularly in areas related to community well-being, policy development, and governance. Some sample use cases include:\\n\"\n",
    "    \"- **Community Well-being**: Summarizing public health articles to provide timely updates and recommendations for community health and safety.\\n\"\n",
    "    \"- **Policy Development**: Analyzing and summarizing policy documents to identify key issues and trends, facilitating evidence-based decision-making.\\n\"\n",
    "    \"- **Governance**: Summarizing news articles and reports related to government activities and public affairs to monitor public sentiment and inform policy responses.\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
